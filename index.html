<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Sabrina Boyle Website</title>
    </head>
    <body>
        <h1>Sabrina Boyle</h1>
        <h2>Report</h2>
        <p>	In the past week of beginning a literature review, I familiarized myself with the technology currently being applied in the area of sign language recognition. This can be simply broken down into the models used for training, the methods for recognition, and the final application. I also briefly looked into collections of signing data, taking into account the diversity and consent of the signing participants.
	    </p>
        <p> There are three main models mentioned in the papers: CNN, LSTM, and Transformer. From the study by Adeyanju et al [2021], the Convolution neural network (CNN) uses convolution layers to alter images, resulting in the ability to isolate features and patterns. This has been used for SLR models since 2018 and has shown high success rates in testing real-time data. CNN does not need a lot of data for training. The use of Long Short-Term Memory (LSTM) in sign language recognition has produced higher accuracy rates when combined with a CNN or RNN. The benefit of the LSTM is it can keep track of valuable information from conversation history which can aid in continuous sign language recognition. The study by Pu et al [2024] explained that transformers are models relying on attention mechanisms to distinguish features using internal temporal and spatial relationships. Overall, SLRs have shown improved success with various combinations of models rather than one on its own. [1,3]
	    </p>
        <p> While the models help to make predictions, the capturing and storing of signing data can be completed in a few different ways. The first, and most common, is doing a skeleton-based model. This splits the image into key points linked together that form a skeleton. The skeleton method can help with distinguishing between variations in lighting, background, and signer appearance. Another method is RGB-based which uses color data from rgb pixels to account for the whole image or video. The advantages of this method is the application of non-manual markers and increased precision of movement. A newly proposed method to improve the accuracy of the previous ones is called StepNet, which combines part of spatial modeling and part of temporal modeling. The spatial side accounts for appearance-based properties in the feature space: hands, face, body. The temporal side uses LSTM to highlight relevant attributes over time. [5]
	    </p>
        <p> While sign language recognition has become increasingly popular in the machine learning industry, there are some common consistencies in applications. They tend to follow the route of either game-based ASL learning platforms or engineered wearables for facilitating the ease of communication. [2,4]
	    </p>
        <p> There have been multiple efforts on data collection for sign language recognition. The most notable efforts are the work from ASL Citizen, Desai et al [2023], which collected community sourced data with a diverse cast of signers in a variety of situations. Another study by Kezar et al [2023] created an aggregate collection using ASL-LEX, SignBank and ASL Citizen, called Sem-Lex Benchmark. All of this data was collected with the informed consent of the signers. [6,7]
        </p>

        <h2>References</h2>
        <ol>
            <li>I. A. Adeyanju, O. O. Bello, and M. A. Adegboye. 2021. Machine learning methods for sign language recognition: A critical review and analysis. Intelligent Systems with Applications 12, (November 2021), 200056. <a href="https://doi.org/10.1016/j.iswa.2021.200056"> https://doi.org/10.1016/j.iswa.2021.200056</a></li>
            <li>Yincheng Jin, Seokmin Choi, Yang Gao, Jiyang Li, Zhengxiong Li, and Zhanpeng Jin. 2023. TransASL: A Smart Glass based Comprehensive ASL Recognizer in Daily Life. In Proceedings of the 28th International Conference on Intelligent User Interfaces, March 27, 2023. ACM, Sydney NSW Australia, 802–818. <a href="https://doi.org/10.1145/3581641.3584071">https://doi.org/10.1145/3581641.3584071</a></li>
            <li>Muxin Pu, Mei Kuan Lim, and Chun Yong Chong. 2024. Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign Language Recognition. In Proceedings of the 32nd ACM International Conference on Multimedia (MM ’24), October 28, 2024. Association for Computing Machinery, New York, NY, USA, 9387–9396. <a href="https://doi.org/10.1145/3664647.3681578">https://doi.org/10.1145/3664647.3681578</a></li>
            <li>Ayush Sharma, Dongping Guo, Arsh Parmar, Jianye Ge, and Hongmin Li. 2024. Promoting Sign Language Awareness: A Deep Learning Web Application for Sign Language Recognition. In Proceedings of the 2024 8th International Conference on Deep Learning Technologies (ICDLT ’24), November 13, 2024. Association for Computing Machinery, New York, NY, USA, 22–28. <a href="https://doi.org/10.1145/3695719.3695723">https://doi.org/10.1145/3695719.3695723</a></li>
            <li>Xiaolong Shen, Zhedong Zheng, and Yi Yang. 2024. StepNet: Spatial-temporal Part-aware Network for Isolated Sign Language Recognition. ACM Trans. Multimedia Comput. Commun. Appl. 20, 7 (May 2024), 226:1-226:19. <a href="https://doi.org/10.1145/3656046">https://doi.org/10.1145/3656046</a></li>
            <li>Aashaka Desai, Lauren Berger, Fyodor O. Minakov, Vanessa Milan, Chinmay Singh, Kriston Pumphrey, Richard E. Ladner, Hal Daumé III, Alex X. Lu, Naomi Caselli, and Danielle Bragg. 2023. ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. <a href="https://doi.org/10.48550/arXiv.2304.05934">https://doi.org/10.48550/arXiv.2304.05934</a></li>
            <li>Lee Kezar, Jesse Thomason, Naomi Caselli, Zed Sehyr, and Elana Pontecorvo. 2023. The Sem-Lex Benchmark: Modeling ASL Signs and their Phonemes. In Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’23), October 22, 2023. Association for Computing Machinery, New York, NY, USA, 1–10. <a href="https://doi.org/10.1145/3597638.3608408">https://doi.org/10.1145/3597638.3608408</a></li>
        </ol>
    </body>
</html>
